{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6d55c6-c3c1-4f35-952b-162664c95452",
   "metadata": {},
   "source": [
    "# Part 1. Data Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d18715-b361-426a-b069-f29e1c85e68b",
   "metadata": {},
   "source": [
    "### Data is coming from around 400 stocks of miniute-level information from 2023/4/1 to 2025/4/1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5f11f-3996-48ea-9463-577bc5b0138c",
   "metadata": {},
   "source": [
    "### Check the exact numbers of csv files we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abadf73a-40ee-4823-a543-5401e36ecff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total csv files: 428\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project')\n",
    "print(f\"Total csv files: {len([f for f in files if f.endswith('.csv')])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dbc96-28fa-48fe-9904-ed6b261e04d1",
   "metadata": {},
   "source": [
    "### Merge all csv to one for the convenience of following processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1554c0c-63b6-483e-a46c-9f07ee7b1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\Username\\OneDrive\\Desktop\\810project'\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith(\".csv\")]\n",
    "\n",
    "all_data = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(path, file))\n",
    "    ticker = file.replace(\".csv\", \"\")\n",
    "    df[\"ticker\"] = ticker\n",
    "    all_data.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_data, ignore_index=True)\n",
    "merged_df.to_csv(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\merged_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c52d0b-5ec8-45e0-877b-70da92d05167",
   "metadata": {},
   "source": [
    "### Too slow, transfrom to Parquet to improve efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d56514-056a-41c7-a6a7-6d4df94e5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_parquet(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\merged_raw.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bd94f-e3a8-430f-84af-c1bb34c854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_par = pd.read_parquet(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\mermiss_raw.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066a0bc-764e-44cf-9e05-6a712ab17d88",
   "metadata": {},
   "source": [
    "### Simply view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cecb2-21a3-4467-97ac-6cbc4f030e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a42b9-558b-4051-80ee-e8748b686caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5307cd1-fab7-4610-80f9-f49f2a372901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8faa8-c138-4bd2-a11f-6a7010438a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9904900-9ec0-498d-81bc-abcdd598a13e",
   "metadata": {},
   "source": [
    "### Rename the columns name for better viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bcaf6-b68b-4b98-95f0-cae117b63fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par = df_par.rename(columns={\n",
    "    'v': 'volume',\n",
    "    'vw': 'vwap',\n",
    "    'o': 'open',\n",
    "    'c': 'close',\n",
    "    'h': 'high',\n",
    "    'l': 'low',\n",
    "    't': 'timestamp',\n",
    "    'n': 'transactions'\n",
    "})\n",
    "df_par.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523948c0-be4a-45cc-a331-029dbc5b4f76",
   "metadata": {},
   "source": [
    "### We are starting to deal with the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4ca0b-82f5-434a-ba8c-3db7dd6a3feb",
   "metadata": {},
   "source": [
    "### Since the data is min-level stock info, for the price columns, we use ffill to deal with\n",
    "### Starting as a quant, we need to ensure the truth of data, so just apply ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f81fef-f427-4d10-afe6-59d942358189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the price cols \n",
    "price_cols = ['open', 'close', 'high', 'low']\n",
    "\n",
    "# use for loop to ffill\n",
    "for col in price_cols:\n",
    "    df_par[col] = df_par.groupby('ticker')[col].ffill()\n",
    "\n",
    "# check again the data\n",
    "df_par.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3372b9-2ced-407e-bb8b-9b03c758e1ea",
   "metadata": {},
   "source": [
    "### For the volume, just fill 0 since it does not matter\n",
    "### For the volume weighted average price, we apply linear interpoate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc089c-1444-4c69-9ffb-8e16ff4e32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par['volume'] = df_par['volume'].fillna(0)\n",
    "\n",
    "# the vwap need to interpolate in the same ticker\n",
    "df_par['vwap'] = df_par.groupby('ticker')['vwap'].transform(lambda group: group.interpolate(method='linear'))\n",
    "\n",
    "df_par.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c6c6c-54ad-424b-970e-cd84d14cddca",
   "metadata": {},
   "source": [
    "### Now the data seems good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031cae64-9552-4d37-8987-92cd9c423a08",
   "metadata": {},
   "source": [
    "### Save the clean df so that we do not need to clean again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cff042-bc34-420c-b04f-be6e1b8a6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.to_parquet(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\merged_cleaned.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f0113-aec0-48a7-ba48-ea99176793d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_par = pd.read_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5c4a0-fae3-4ab1-ab27-bab29caef82e",
   "metadata": {},
   "source": [
    "### Use spark to do data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1edd7be-1017-4fdc-8836-d1222db19597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"810Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"24g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84ea2018-1a48-46ba-a264-0309ad19405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v: double (nullable = true)\n",
      " |-- vw: double (nullable = true)\n",
      " |-- o: double (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- h: double (nullable = true)\n",
      " |-- l: double (nullable = true)\n",
      " |-- t: long (nullable = true)\n",
      " |-- n: long (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      "\n",
      "+-------+--------+-------+--------+-------+------+-------------+---+-------------------+------+\n",
      "|      v|      vw|      o|       c|      h|     l|            t|  n|           datetime|ticker|\n",
      "+-------+--------+-------+--------+-------+------+-------------+---+-------------------+------+\n",
      "|17632.0|137.0174| 137.42|  137.26|137.565|136.85|1680528600000|105|2023-04-03 13:30:00|     A|\n",
      "| 1011.0| 137.253|137.255|  137.32| 137.32|137.25|1680528660000| 28|2023-04-03 13:31:00|     A|\n",
      "|  697.0|137.2364| 137.22|  137.33| 137.33|136.95|1680528720000| 18|2023-04-03 13:32:00|     A|\n",
      "| 4004.0|137.3055| 136.99|  137.57| 137.57|136.99|1680528780000| 79|2023-04-03 13:33:00|     A|\n",
      "| 7244.0|137.4399| 137.69|137.3784| 137.72|137.11|1680528840000|107|2023-04-03 13:34:00|     A|\n",
      "| 2483.0|136.7879|137.215|  136.61|137.215|136.61|1680528900000| 74|2023-04-03 13:35:00|     A|\n",
      "| 4375.0|136.4643| 136.53| 136.715|136.715|136.34|1680528960000| 72|2023-04-03 13:36:00|     A|\n",
      "| 4552.0|136.6168| 136.67|  136.47| 136.87|136.28|1680529020000| 80|2023-04-03 13:37:00|     A|\n",
      "| 1157.0|136.4493| 136.32| 136.445|136.465|136.32|1680529080000| 13|2023-04-03 13:38:00|     A|\n",
      "| 4688.0|136.6723| 136.58|  136.98| 136.98|136.55|1680529140000| 71|2023-04-03 13:39:00|     A|\n",
      "+-------+--------+-------+--------+-------+------+-------------+---+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.parquet(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\mermiss_raw.parquet')\n",
    "df_spark.printSchema()\n",
    "df_spark.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96eabfee-36e1-4447-b665-3031bd8a5872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+--------+-------+------+-------------+------------+-------------------+------+\n",
      "| volume|    vwap|   open|   close|   high|   low|    timestamp|transactions|           datetime|ticker|\n",
      "+-------+--------+-------+--------+-------+------+-------------+------------+-------------------+------+\n",
      "|17632.0|137.0174| 137.42|  137.26|137.565|136.85|1680528600000|         105|2023-04-03 13:30:00|     A|\n",
      "| 1011.0| 137.253|137.255|  137.32| 137.32|137.25|1680528660000|          28|2023-04-03 13:31:00|     A|\n",
      "|  697.0|137.2364| 137.22|  137.33| 137.33|136.95|1680528720000|          18|2023-04-03 13:32:00|     A|\n",
      "| 4004.0|137.3055| 136.99|  137.57| 137.57|136.99|1680528780000|          79|2023-04-03 13:33:00|     A|\n",
      "| 7244.0|137.4399| 137.69|137.3784| 137.72|137.11|1680528840000|         107|2023-04-03 13:34:00|     A|\n",
      "+-------+--------+-------+--------+-------+------+-------------+------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_dict = {\n",
    "    \"v\": \"volume\",\n",
    "    \"vw\": \"vwap\",\n",
    "    \"o\": \"open\",\n",
    "    \"c\": \"close\",\n",
    "    \"h\": \"high\",\n",
    "    \"l\": \"low\",\n",
    "    \"t\": \"timestamp\",\n",
    "    \"n\": \"transactions\"\n",
    "}\n",
    "for old_name, new_name in rename_dict.items():\n",
    "    df_spark = df_spark.withColumnRenamed(old_name, new_name)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09dc63ae-5e89-48af-b2e1-adbff6c55c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+----------+-----------+---------+\n",
      "|null_open|null_high|null_low|null_close|null_volume|null_vwap|\n",
      "+---------+---------+--------+----------+-----------+---------+\n",
      "|      218|      216|     219|       198|        209|      190|\n",
      "+---------+---------+--------+----------+-----------+---------+\n",
      "\n",
      "+------+---+------------+\n",
      "|ticker|cnt|total_volume|\n",
      "+------+---+------------+\n",
      "+------+---+------------+\n",
      "\n",
      "+------+---+-------------+\n",
      "|ticker|cnt|null_vwap_cnt|\n",
      "+------+---+-------------+\n",
      "+------+---+-------------+\n",
      "\n",
      "root\n",
      " |-- volume: double (nullable = true)\n",
      " |-- vwap: double (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- transactions: long (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|unique_ticker_count|\n",
      "+-------------------+\n",
      "|                429|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_spark.createOrReplaceTempView(\"market\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    SUM(CASE WHEN open IS NULL THEN 1 ELSE 0 END) AS null_open,\n",
    "    SUM(CASE WHEN high IS NULL THEN 1 ELSE 0 END) AS null_high,\n",
    "    SUM(CASE WHEN low IS NULL THEN 1 ELSE 0 END) AS null_low,\n",
    "    SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) AS null_close,\n",
    "    SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) AS null_volume,\n",
    "    SUM(CASE WHEN vwap IS NULL THEN 1 ELSE 0 END) AS null_vwap\n",
    "FROM market\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT ticker, COUNT(*) AS cnt, SUM(volume) AS total_volume\n",
    "FROM market\n",
    "GROUP BY ticker\n",
    "HAVING total_volume = 0\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT ticker, COUNT(*) AS cnt,\n",
    "       SUM(CASE WHEN vwap IS NULL THEN 1 ELSE 0 END) AS null_vwap_cnt\n",
    "FROM market\n",
    "GROUP BY ticker\n",
    "HAVING null_vwap_cnt = cnt\n",
    "\"\"\").show()\n",
    "\n",
    "df_spark.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(DISTINCT ticker) AS unique_ticker_count FROM market\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86c03e0-d895-4e15-83e0-50530b50c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import last, col\n",
    "import sys\n",
    "\n",
    "w = Window.partitionBy(\"ticker\").orderBy(\"datetime\").rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "# forward fill open/high/low/close\n",
    "for colname in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "    df_spark = df_spark.withColumn(colname, last(col(colname), ignorenulls=True).over(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35e789a3-67b1-482b-9dc9-795903608ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+-----+-------------+------------+-------------------+------+---------+---------+-------+\n",
      "| volume|  open| close|  high|  low|    timestamp|transactions|           datetime|ticker|vwap_prev|vwap_next|   vwap|\n",
      "+-------+------+------+------+-----+-------------+------------+-------------------+------+---------+---------+-------+\n",
      "|  284.0|  85.5|  85.5|  85.5| 85.5|1680519600000|          16|2023-04-03 11:00:00|   GIS|     NULL|  85.3907|85.5081|\n",
      "|  713.0| 85.37| 85.37| 85.37|85.37|1680528540000|          17|2023-04-03 13:29:00|   GIS|  85.5081|  85.4951|85.3907|\n",
      "|85741.0| 85.49|85.555| 85.65|85.43|1680528600000|         275|2023-04-03 13:30:00|   GIS|  85.3907|  85.5402|85.4951|\n",
      "| 6811.0| 85.57| 85.47| 85.66|85.45|1680528660000|         127|2023-04-03 13:31:00|   GIS|  85.4951|   85.393|85.5402|\n",
      "| 3919.0|85.475| 85.36|85.475|85.35|1680528720000|          95|2023-04-03 13:32:00|   GIS|  85.5402|  85.2405| 85.393|\n",
      "+-------+------+------+------+-----+-------------+------------+-------------------+------+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, lead, when\n",
    "\n",
    "df_spark = df_spark.fillna({'volume': 0})\n",
    "\n",
    "w = Window.partitionBy(\"ticker\").orderBy(\"datetime\")\n",
    "# linear interpolation for vwap (very simplified)\n",
    "df_spark = df_spark.withColumn(\"vwap_prev\", lag(\"vwap\", 1).over(w))\n",
    "df_spark = df_spark.withColumn(\"vwap_next\", lead(\"vwap\", 1).over(w))\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"vwap_interp\",\n",
    "    when(col(\"vwap\").isNull(), (col(\"vwap_prev\") + col(\"vwap_next\")) / 2).otherwise(col(\"vwap\"))\n",
    ").drop(\"vwap\").withColumnRenamed(\"vwap_interp\", \"vwap\")\n",
    "\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2a279-b77c-4f75-be1b-3892576c4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.mode(\"overwrite\").parquet(\"merged_cleaned_spark.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "746e11d4-d3eb-4dfc-a9b8-3703b29a9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f75fc3-9b0a-408c-bd9d-4a96e231d323",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a8e1f-c708-4059-b19d-e87c1145b79d",
   "metadata": {},
   "source": [
    "### Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb991337-9277-4d09-bc13-7e69c2516a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par[\"datetime\"] = pd.to_datetime(df_par[\"datetime\"])\n",
    "\n",
    "df_par[\"minute\"] = df_par[\"datetime\"].dt.minute\n",
    "df_par[\"hour\"] = df_par[\"datetime\"].dt.hour\n",
    "df_par[\"dayofweek\"] = df_par[\"datetime\"].dt.dayofweek\n",
    "df_par[\"is_open_hour\"] = df_par[\"hour\"].between(9, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc480da-3578-4e11-a075-fa5eeeec91cd",
   "metadata": {},
   "source": [
    "### Price Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f6481-e673-4eae-83c8-76bd5660ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_par[\"hl_spread\"] = df_par[\"high\"] - df_par[\"low\"]\n",
    "df_par[\"oc_return\"] = (df_par[\"close\"] - df_par[\"open\"]) / df_par[\"open\"]\n",
    "\n",
    "df_par[\"log_return\"] = (\n",
    "    df_par.groupby(\"ticker\")[\"close\"]\n",
    "    .apply(lambda x: np.log(x / x.shift(1)))\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd551c-a152-47a2-a732-884041e26801",
   "metadata": {},
   "source": [
    "### Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49443b42-772e-4067-88cc-4ae2c4754719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_par[\"close_lag1\"] = (\n",
    "    df_par.groupby(\"ticker\")[\"close\"]\n",
    "    .shift(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "df_par[\"close_roll_mean_5\"] = (\n",
    "    df_par.groupby(\"ticker\")[\"close\"]\n",
    "    .transform(lambda x: x.rolling(5).mean())\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "df_par[\"volume_roll_std_15\"] = (\n",
    "    df_par.groupby(\"ticker\")[\"volume\"]\n",
    "    .transform(lambda x: x.rolling(15).std())\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6136e86-b4b9-40e6-905b-363ca4dd7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0383c64-f95b-4d34-9fe6-ff583c1953ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par.to_parquet(r'C:\\Users\\Username\\OneDrive\\Desktop\\810project\\dffeatures.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ee360-c0e2-44e8-824e-a727aded6273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
